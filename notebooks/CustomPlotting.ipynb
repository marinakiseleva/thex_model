{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functionality to combine visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "%matplotlib inline  \n",
    "\n",
    "import numpy as np  \n",
    "from matplotlib import rc\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "from models.binary_model.binary_model import BinaryModel\n",
    "from models.ind_model.ind_model import OvAModel\n",
    "from models.multi_model.multi_model import MultiModel\n",
    "\n",
    "from thex_data.data_consts import *\n",
    "from mainmodel.helper_compute import *\n",
    "from mainmodel.helper_plotting import *\n",
    "from utilities import utilities as thex_utils\n",
    "\n",
    "\n",
    "mags = [\"g_mag\",  \"r_mag\", \"i_mag\", \"z_mag\", \"y_mag\",\n",
    "        \"W1_mag\", \"W2_mag\",\n",
    "        \"J_mag\", \"K_mag\", \"H_mag\"]\n",
    "\n",
    "EXPS_DIR = ROOT_DIR + \"/../../exps/v8_db_runs/reg_runs/\"\n",
    "\n",
    "codes= [\"A1\", \"F1\", \"B1\", \"G1\"]\n",
    " \n",
    "# rc('text', usetex=True)\n",
    "mpl.rcParams['font.serif'] = ['times', 'times new roman']\n",
    "mpl.rcParams['font.family'] = 'serif'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi \n",
    "multi_model = MultiModel(cols = mags,\n",
    "                       folds = 10,\n",
    "                       transform_features = True, \n",
    "                       case_code = codes,\n",
    "                       balanced_purity = True) \n",
    "multi_model=load_prev_exp(EXPS_DIR, \"Multiclass_Classifier1/\", multi_model)\n",
    "\n",
    "\n",
    "# Binary\n",
    "model2 = BinaryModel(cols = mags,\n",
    "                       folds = 10,\n",
    "                       transform_features = True, \n",
    "                       case_code = codes,\n",
    "                       balanced_purity = True)\n",
    "binary_model = load_prev_exp(EXPS_DIR, \"Binary_Classifiers2/\", model2)\n",
    "\n",
    "\n",
    "# OvA \n",
    "model = OvAModel(cols = mags,\n",
    "                 folds = 10,\n",
    "                 transform_features = True, \n",
    "                 case_code = codes,\n",
    "                 balanced_purity = True)\n",
    "\n",
    "ova_model = load_prev_exp(EXPS_DIR, \"OVA_Classifier1/\", model) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge purity/comp average plots\n",
    "\n",
    "Below is the code to merge the OVA and multi KDE purity/comp plots; and to visualize the binary classifiers' average balanced purity and completeness in a similar fashion, separately.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_metrics_ax(model, pur_ax, comp_ax):\n",
    "    c_baselines, p_baselines = compute_baselines(\n",
    "        model.class_counts, \n",
    "        model.class_labels,\n",
    "        model.get_num_classes(), \n",
    "        model.balanced_purity,  \n",
    "        model.class_priors)\n",
    "    pc_per_trial = model.get_pc_per_trial(model.results)\n",
    "    ps, cs = model.get_pc_performance(pc_per_trial)\n",
    "    p_intvls, c_intvls =compute_confintvls(pc_per_trial, model.class_labels, model.balanced_purity)\n",
    "    \n",
    "    y_indices, class_names=model.plot_metrics_ax(pur_ax, ps, \"Balanced Purity\", p_baselines, p_intvls)\n",
    "\n",
    "    y_indices, class_names=model.plot_metrics_ax(comp_ax, cs, \"Completeness\", c_baselines, c_intvls)\n",
    "    return y_indices, class_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot Multi & OVA\n",
    "f, ax = plt.subplots(nrows=2, ncols=2,\n",
    "                     sharex=True, sharey=True,\n",
    "                     figsize=(8,8),  dpi=600)\n",
    "rc('text', usetex=True)\n",
    "mpl.rcParams['font.serif'] = ['times', 'times new roman']\n",
    "mpl.rcParams['font.family'] = 'serif'\n",
    "print(\"\\n --------------------------- ova_model--------------------------- \\n\\n\")\n",
    "get_metrics_ax(ova_model, ax[0][0], ax[0][1]) \n",
    "\n",
    "print(\"\\n --------------------------- multi_model--------------------------- \\n\\n\")\n",
    "y_indices, class_names=get_metrics_ax(multi_model, ax[1][0], ax[1][1]) \n",
    "\n",
    "ax[0][0].tick_params(direction=\"in\")\n",
    "ax[0][1].tick_params(direction=\"in\")\n",
    "ax[0][1].text(1.05,2.1, \"OVA\", fontsize=20)\n",
    "ax[1][1].text(1.05,2.1, \"Multiclass\\nKDE\", fontsize=20)\n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "ax[0][0].set_yticks(y_indices)\n",
    "ax[0][0].set_yticklabels(clean_class_names(class_names),\n",
    "           fontsize=16,  horizontalalignment='right')\n",
    "ax[1][0].set_yticks(y_indices)\n",
    "ax[1][0].set_yticklabels(clean_class_names(class_names),\n",
    "           fontsize=16,  horizontalalignment='right')\n",
    "plt.xticks(np.linspace(0,1,10, endpoint=False))\n",
    "\n",
    "ax[1][0].set_xlabel(\"Balanced Purity (\\%)\", fontsize=TICK_S)\n",
    "ax[1][1].set_xlabel(\"Completeness (\\%)\", fontsize=TICK_S)\n",
    "plt.savefig(\"../output/custom_figures/multis_metrics.pdf\", bbox_inches='tight')\n",
    "plt.show()\n",
    "#  Binary\n",
    "f, ax = plt.subplots(nrows=1, ncols=2,\n",
    "                     sharex=True, sharey=True,\n",
    "                     figsize=(6,3),  dpi=600)\n",
    "rc('text', usetex=True)\n",
    "mpl.rcParams['font.serif'] = ['times', 'times new roman']\n",
    "mpl.rcParams['font.family'] = 'serif'\n",
    " \n",
    "y_indices, class_names=get_metrics_ax(model = binary_model, pur_ax = ax[0], comp_ax = ax[1] )\n",
    "\n",
    "ax[0].tick_params(direction=\"in\")\n",
    "ax[1].tick_params(direction=\"in\")\n",
    "\n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "\n",
    "ax[0].set_yticks(y_indices)\n",
    "ax[0].set_yticklabels(clean_class_names(class_names),\n",
    "           fontsize=14,  horizontalalignment='right')\n",
    "plt.xticks(np.linspace(0,1,10, endpoint=False))\n",
    "ax[0].set_xlabel(\"Balanced Purity (\\%)\", fontsize=14)\n",
    "ax[1].set_xlabel(\"Completeness (\\%)\", fontsize=14)\n",
    "plt.savefig(\"../output/custom_figures/binary_metrics.pdf\", bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot probability plots together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Empirical Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These calls are for the old way of generating empirical prob plots - when they were not balanced.\n",
    "# Now, I call the new functions from within plot_rates_together\n",
    "\n",
    "# ova_model.range_metrics = ova_model.compute_probability_range_metrics(\n",
    "#         ova_model.results, bin_size=0.2)\n",
    "# binary_model.range_metrics = binary_model.compute_probability_range_metrics(\n",
    "#         binary_model.results, bin_size=0.2)\n",
    "# multi_model.range_metrics = multi_model.compute_probability_range_metrics(\n",
    "#         multi_model.results, bin_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_rates_together(binary_model, ova_model, multi_model, indices=[0,1,2,3,4,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_rates_together(binary_model, ova_model, multi_model, indices=[6,7,8,9,10,11])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below has been commented out becaues we no longer use this empirical-probability callout figure in the paper. Since these plots have been updated to be 'balanced' they lose their sense of calibration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cur_model.class_labels\n",
    "# # ia, ic, iib, iin, tde\n",
    "# multi_model_preds = np.concatenate(multi_model.results)\n",
    "# multi_model.class_prob_rates = get_multi_emp_prob_rates(multi_model_preds,\n",
    "#                                                             multi_model.class_labels,\n",
    "#                                                             0.2,\n",
    "#                                                             multi_model.class_counts)\n",
    "# cur_model = multi_model\n",
    "# # cur_model.range_metrics = cur_model.compute_probability_range_metrics(\n",
    "# #         cur_model.results, bin_size=0.2)\n",
    "\n",
    "# # call outs for multi only\n",
    "# indices = [0, 4, 7, 10,11]\n",
    "# rc('text', usetex=True)\n",
    "# cur_model = multi_model\n",
    "\n",
    "\n",
    "# class_labels = cur_model.class_labels \n",
    "\n",
    "# num_classes = len(indices)\n",
    "\n",
    "# f, ax = plt.subplots(nrows=1,\n",
    "#                      ncols=len(indices),\n",
    "#                      sharex=True, sharey=True,\n",
    "#                      figsize=(8, 1.9),\n",
    "#                      dpi=280)\n",
    "# row_index = 0\n",
    "# for index in indices: \n",
    "#     cn = cur_model.class_labels[index]\n",
    "#     plot_model_rates(cn, cur_model, ax[row_index])  \n",
    "#     ax[row_index].text(-0.45, 0.81, clean_class_name(cn), fontsize=14)\n",
    "# #     plot_model_rates(cn, ova_model, ax[row_index][1])\n",
    "#     row_index+=1\n",
    "    \n",
    "# y_indices = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "# y_ticks = [\"10\", \"30\", \"50\", \"70\", \"90\"]\n",
    "# # x and y indices/ticks are the same\n",
    "# plt.xticks(np.arange(5), y_ticks)\n",
    "# plt.yticks(y_indices, y_ticks)\n",
    "# plt.rc('xtick', labelsize=10)\n",
    "# plt.rc('ytick', labelsize=10)\n",
    "\n",
    "# mpl.rcParams['font.serif'] = ['times', 'times new roman']\n",
    "# mpl.rcParams['font.family'] = 'serif'\n",
    "\n",
    "# f.text(0.5, -0.07, 'Assigned Probability ' + r' $\\pm10\\%$', fontsize=14, ha='center')\n",
    "# f.text(0.05, .5, r'Empirical Prob. ($\\%$)',\n",
    "#        fontsize=14, va='center', rotation='vertical')\n",
    "# plt.subplots_adjust(wspace=0, hspace=0)\n",
    "# f.savefig(ROOT_DIR + \"/output/custom_figures/prob_callouts.pdf\", bbox_inches='tight')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Purity /completeness curves vs probability threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_pc_curves_together(binary_model, ova_model, multi_model, indices=[0,1,2,3,4,5])\n",
    "plot_pc_curves_together(binary_model, ova_model, multi_model, indices=[6,7,8,9,10,11])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pc_curves_together(binary_model, ova_model, multi_model, indices=[1,2,6,8,9,11])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining with vs without priors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Trials of K-Fold Cross Validation\n",
    "Combine X trials for with and without priors, get average meaure and confidence intervals.\n",
    "Currently run 6-fold cross validation 10 times and save results & y of each experiment in a list, agg_results.pickle.\n",
    "\n",
    "*******Stuff below here isn't working great."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GET_DIR = ROOT_DIR + \"/../../exps/v8_db_runs/new_lsst_tests/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_no_priors = MultiModel(cols = mags,\n",
    "                       folds = 6,\n",
    "                       transform_features = True, \n",
    "                       case_code = codes,\n",
    "                       balanced_purity = True,\n",
    "                            lsst_test=True) \n",
    "multi_no_priors=load_prev_exp(GET_DIR, \"Multiclass_Classifier1/\", multi_no_priors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_priors = MultiModel(cols = mags,\n",
    "                       folds = 6,\n",
    "                       transform_features = True, \n",
    "                       case_code = codes,\n",
    "                       balanced_purity = True,\n",
    "                            lsst_test=True,\n",
    "                            priors=True) \n",
    "multi_priors=load_prev_exp(GET_DIR, \"Multiclass_Classifier2/\", multi_no_priors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_EPs_Priors(Model_NP=multi_no_priors, Model_WP=multi_priors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_EPs_Priors(Model_NP, Model_WP):\n",
    "    \"\"\"\n",
    "    Plot Empirical Probability Plots (EPs) for with priors vs no-priors\n",
    "    \"\"\"\n",
    "    rc('text', usetex=True) \n",
    "    mpl.rcParams['font.serif'] = ['times', 'times new roman']\n",
    "    mpl.rcParams['font.family'] = 'serif'\n",
    "    \n",
    "    class_labels = ['Unspecified Ia', 'Ia-91bg', 'Ibc', 'II', 'SLSN-I', 'TDE'] \n",
    "    f, ax = plt.subplots(nrows=len(class_labels),\n",
    "                         ncols=2,\n",
    "                         sharex=True, sharey=True,\n",
    "                         figsize=(FIG_WIDTH, 9),\n",
    "                         dpi=DPI)\n",
    "    plot_index = 0\n",
    "    for class_index in range(len(class_labels)):\n",
    "        if plot_index == 0:\n",
    "            # Add titles to top of plots\n",
    "            ax[plot_index][0].set_title(\"Uniform Priors\", fontsize=14)\n",
    "            ax[plot_index][1].set_title(\"Frequency-based Priors\", fontsize=14)\n",
    "\n",
    "    \n",
    "    \n",
    "        ModelMets_NP_preds = np.concatenate(Model_NP.results)\n",
    "        ModelMets_NP.class_prob_rates = get_multi_emp_prob_rates(ModelMets_NP_preds,\n",
    "                                                              Model_NP.class_labels,\n",
    "                                                              0.2,\n",
    "                                                              Model_NP.class_counts)\n",
    "        \n",
    "        ModelMets_WP_preds = np.concatenate(Model_WP.results)\n",
    "        ModelMets_WP.class_prob_rates = get_multi_emp_prob_rates(ModelMets_WP_preds,\n",
    "                                                              Model_WP.class_labels,\n",
    "                                                              0.2,\n",
    "                                                              Model_WP.class_counts)\n",
    "        \n",
    "        class_name = class_labels[class_index]  \n",
    "        plot_model_rates(class_name, ModelMets_NP, ax[plot_index][0])\n",
    "        plot_model_rates(class_name, ModelMets_WP, ax[plot_index][1])\n",
    "\n",
    "        pretty_class_name = clean_class_name(class_name)\n",
    "        ax[plot_index][0].text(-0.2, 0.8, pretty_class_name, fontsize=16) \n",
    "        plot_index += 1\n",
    "\n",
    "    y_indices = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "    y_ticks = [\"10\", \"30\", \"50\", \"70\", \"90\"]\n",
    "    # x and y indices/ticks are the same\n",
    "    plt.xticks(np.arange(5), y_ticks)\n",
    "    plt.yticks(y_indices, y_ticks)\n",
    "    plt.rc('xtick', labelsize=14)\n",
    "    plt.rc('ytick', labelsize=14)\n",
    "\n",
    "    f.text(0.5, 0.06, 'Assigned Probability' + r' $\\pm$10\\%', fontsize=14, ha='center')\n",
    "    f.text(0.02, .5, r'Empirical Probability $\\equiv$ P/Total ($\\%$)',\n",
    "               fontsize=14, va='center', rotation='vertical')\n",
    "\n",
    "    plt.subplots_adjust(wspace=0, hspace=0)\n",
    "\n",
    "    f.savefig(ROOT_DIR + \"/output/custom_figures/merged_metrics_priors_comp_AGG.pdf\", bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define class to keep track of all relevant model mets and details\n",
    "\n",
    "class ModelsMets:\n",
    "    def __init__(self, with_priors):\n",
    "        \"\"\"\n",
    "        Keep track of variables\n",
    "        :param with_priors: Boolean for whether this model uses frequency-based priors (True) or uniform (False)\n",
    "        \"\"\"\n",
    "        # Pull down all data\n",
    "        # Lists of 10 elements. Each element is results/y for that run of 6-fold CV. \n",
    "        all_results, all_y = self.get_agg_data(WP=with_priors) \n",
    "            \n",
    "        self.all_results = all_results\n",
    "        self.all_y = all_y\n",
    "        \n",
    "        model = MultiModel(cols = mags,\n",
    "                       folds = 6,\n",
    "                       min_class_size = 3,\n",
    "                       max_class_size = 4800,\n",
    "                       transform_features = True,\n",
    "                       case_code = codes,\n",
    "                       lsst_test= True,\n",
    "                       priors =with_priors)\n",
    "        \n",
    "        self.model = model\n",
    "        \n",
    "        \n",
    "#         # Lists of length 10, containing maps of purity/comp for each run\n",
    "#         self.all_purities, self.all_comps = self.get_all_measures()\n",
    "# #         Map from class name to list of length for avg purity for each trial\n",
    "#         self.class_purities = self.collect_mets(self.all_purities)\n",
    "#         self.class_comps = self.collect_mets(self.all_comps)\n",
    "        \n",
    "    \n",
    "    def get_agg_data(self, WP):\n",
    "        \"\"\"\n",
    "        Get results and y from file \n",
    "        \"\"\"\n",
    "        rname = \"Multiclass_Classifier2/results.pickle\" if WP else \"Multiclass_Classifier1/results.pickle\"\n",
    "        yname = \"Multiclass_Classifier2/y.pickle\" if WP else \"Multiclass_Classifier1/y.pickle\"\n",
    "        with open(GET_DIR + rname, 'rb') as handle:\n",
    "            results = pickle.load(handle)\n",
    "        with open(GET_DIR + yname, 'rb') as handle:\n",
    "            y = pickle.load(handle)\n",
    "        return results, y\n",
    "\n",
    "    def get_all_measures(self):\n",
    "        trialPs = []\n",
    "        trialCs = []\n",
    "        for index, results in enumerate(self.all_results):\n",
    "            y = self.all_y[index]\n",
    "\n",
    "            # Compute performance for this 6-fold set.\n",
    "            pc_per_trial = self.model.get_pc_per_trial(results)\n",
    "            ps, cs = self.model.get_pc_performance(pc_per_trial)\n",
    "            # Get average purity per class\n",
    "            trialPs.append(ps)\n",
    "\n",
    "            # Get average completeness per class\n",
    "            trialCs.append(cs)\n",
    "        return trialPs, trialCs\n",
    "\n",
    "    def collect_mets(self, metSet):\n",
    "        \"\"\"\n",
    "        Convert metSet from list of maps (where each map is class name to list of values) \n",
    "        to map from class name to list of all mets for that class.\n",
    "        :param metSet: List of maps, where each map is set of metrics from that trial\n",
    "        \"\"\"\n",
    "        collMets = {class_name : [] for class_name in self.model.class_labels}\n",
    "        for class_name in self.model.class_labels:\n",
    "            for index, curmetSet in enumerate(metSet):\n",
    "                collMets[class_name].append(curmetSet[class_name])\n",
    "        return collMets\n",
    "    \n",
    "    \n",
    "    def get_all_range_metrics(self):\n",
    "        \"\"\"\n",
    "        Get the range metrics for each trial, saved as list\n",
    "        RMs is [TP, Totals], and all_class_positives is just Ps\n",
    "        \"\"\"\n",
    "        RMs = []\n",
    "        all_class_positives = []\n",
    "        for index, results in enumerate(self.all_results):\n",
    "            # curRM is map {class_name: [tp_range_counts, total_range_counts] }\n",
    "            curRM = self.model.compute_probability_range_metrics(results, bin_size=0.2, concat=True)\n",
    "            #  self.model.class_positives is map {class_name:  pos_class_per_range }\n",
    "            all_class_positives.append(self.model.class_positives)\n",
    "            RMs.append(curRM)\n",
    "            \n",
    "        class_RMs = self.collect_mets(RMs)\n",
    "        class_Ps = self.collect_mets(all_class_positives)\n",
    "        return class_RMs, class_Ps\n",
    "    \n",
    "    def get_avg_range_metrics(self):\n",
    "        \"\"\"\n",
    "        Using the list of range metrics, over each trial, average them. \n",
    "        avg_range_metrics: Ps and Totals in each range.\n",
    "        \"\"\"\n",
    "        class_RMs, class_Ps = self.get_all_range_metrics()\n",
    "        N=10\n",
    "        avg_range_metrics = {class_name : [] for class_name in self.model.class_labels}\n",
    "        class_prob_rates = {class_name : [] for class_name in self.model.class_labels}\n",
    "        for class_name in self.model.class_labels:\n",
    "            class_Positives = np.array([0,0,0,0,0])\n",
    "            class_Totals = np.array([0,0,0,0,0])\n",
    "            all_class_RMs = np.array(class_RMs[class_name])\n",
    "            all_class_Ps = np.array(class_Ps[class_name])\n",
    "            for index, row in enumerate(all_class_RMs):\n",
    "                class_Totals = np.add(class_Totals, row[1])\n",
    "                class_Positives = np.add(class_Positives, all_class_Ps[index])\n",
    "\n",
    "            pos = (class_Positives/ N).astype(int).tolist()\n",
    "            tots = (class_Totals/ N).astype(int).tolist()\n",
    "            avg_range_metrics[class_name] = [pos, tots]\n",
    "            class_prob_rates[class_name] = np.divide(pos, tots)\n",
    "        return avg_range_metrics, class_prob_rates\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average purities and comps across trials and compute conf. intervals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ORDERED_LSST_CLASSES = ['Unspecified Ia', 'Ia-91bg', 'Ibc', 'II', 'SLSN-I', 'TDE']\n",
    "\n",
    "def get_avg(values):\n",
    "    values = np.array(values)\n",
    "    return sum(values) / len(values)\n",
    "\n",
    "def get_cis(values):\n",
    "    \"\"\"\n",
    "    Calculate confidence intervals [µ − 1.96*SEM, µ + 1.96*SEM] where\n",
    "    SEM = σ/sqrt(N) \n",
    "    σ = sqrt( (1/ N ) ∑_n (a_i − µ)^2 )\n",
    "    \"\"\"\n",
    "    values = np.array(values)\n",
    "    mean = get_avg(values)\n",
    "    N = len(values)\n",
    "    stdev = np.sqrt((sum((np.array(values) - mean) ** 2)) / (N - 1))\n",
    "    SEM = stdev / np.sqrt(N)\n",
    "    # 95% confidence intervals, [µ − 1.96σ, µ + 1.96σ]\n",
    "    low_CI = mean - (1.96 * SEM)\n",
    "    high_CI = mean + (1.96 * SEM)\n",
    "    if low_CI < 0:\n",
    "        low_CI = 0\n",
    "    if high_CI > 1:\n",
    "        high_CI = 1\n",
    "    return [low_CI, high_CI]\n",
    "\n",
    "def get_plotting_data(metModel, met_type):\n",
    "    \"\"\"\n",
    "    Format metric data into plotting-ready format, which is lists in order of class names. \n",
    "    Avg, errors, and baselines as lists.\n",
    "    \"\"\"\n",
    "    m = metModel.model\n",
    "    c_baselines, p_baselines = compute_baselines(class_counts=m.class_counts,\n",
    "                                                 class_labels=ORDERED_LSST_CLASSES,\n",
    "                                                 N=m.get_num_classes(),\n",
    "                                                 balanced_purity=False, \n",
    "                                                 class_priors=m.class_priors)\n",
    "    if met_type == \"Purity\":\n",
    "        METS = metModel.class_purities\n",
    "        BASELINES = p_baselines\n",
    "    elif met_type == \"Completeness\":\n",
    "        METS = metModel.class_comps\n",
    "        BASELINES = c_baselines\n",
    "    plot_Means = []\n",
    "    plot_Baselines = []\n",
    "    CIs = []\n",
    "    for cn in ORDERED_LSST_CLASSES: \n",
    "        plot_Means.append(get_avg(METS[cn]))\n",
    "        CIs.append(get_cis(METS[cn]))\n",
    "        plot_Baselines.append(BASELINES[cn])\n",
    "    plot_Errs = prep_err_bars(CIs, plot_Means)\n",
    "    return plot_Means, plot_Errs, plot_Baselines\n",
    "\n",
    "def plot_on_ax(ax, color, indices, metModel, met_type):\n",
    "    \"\"\"\n",
    "    Plot measures on this particular axis, using given model and met_type\n",
    "    :param met_type: Purity or Completeness\n",
    "    \"\"\"\n",
    "    bar_width= 1 / (len(indices)  *2)\n",
    "    plot_Means, plot_Errs, plot_Baselines = get_plotting_data(metModel, met_type)\n",
    "    if color == \"blue\":\n",
    "        label = \"Freq-based Priors\"\n",
    "    else:\n",
    "        label = \"Uniform Priors\"\n",
    "    ax.barh(y=indices, \n",
    "            width=plot_Means, \n",
    "            height=bar_width, \n",
    "            xerr=plot_Errs,\n",
    "            capsize=7, \n",
    "            linewidth=0.5,\n",
    "            edgecolor=BAR_EDGE_COLOR, \n",
    "            ecolor=INTVL_COLOR, \n",
    "            color=color,\n",
    "            label=label)\n",
    "    for index, baseline in enumerate(plot_Baselines):\n",
    "        y_val = indices[index]\n",
    "        ax.vlines(x=baseline,\n",
    "                   ymin=y_val - (bar_width / 2),\n",
    "                   ymax=y_val + (bar_width / 2),\n",
    "                   linewidth=2,\n",
    "               linestyles=(0, (1, 1)), colors=BSLN_COLOR)\n",
    "    \n",
    "    print(\"\\n\" + label + \" stats:\")\n",
    "    for index, cn in enumerate(ORDERED_LSST_CLASSES):\n",
    "        d = (plot_Errs[1][index] -plot_Errs[0][index] )\n",
    "        print(cn + \" \" + str(round(plot_Means[index],4))) # + \" % plus/minus \" + str(d))\n",
    "\n",
    "    \n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_xticks([0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9, 1])\n",
    "    ax.set_xticklabels([\"0\", \"\", \"20\", \"\", \"40\", \"\", \"60\", \"\", \"80\", \"\", \"\"], fontsize=14)\n",
    "    ax.set_xlabel(met_type+\" (\\%)\", fontsize=16)  \n",
    "    \n",
    "\n",
    "def plot_WP_NP_avg_performance(ModelMets_NP, ModelMets_WP):\n",
    "    \"\"\"\n",
    "    Plot average purity and completeness per class for with priors vs no-priors, using ModelMets objets\n",
    "    \"\"\"\n",
    "    f, ax = plt.subplots(nrows=1, ncols=2, sharex=True, sharey=True, figsize=(6,4),  dpi=500)\n",
    "    rc('text', usetex=True)\n",
    "    mpl.rcParams['font.serif'] = ['times', 'times new roman']\n",
    "    mpl.rcParams['font.family'] = 'serif'\n",
    "\n",
    "    indices = np.linspace(0,1,len(ORDERED_LSST_CLASSES))\n",
    "    \n",
    "    bar_width= 1 / (len(indices)  *2)\n",
    "\n",
    "    m2_indices = np.linspace(0,1,len(ORDERED_LSST_CLASSES)+1)\n",
    "    indices = m2_indices + bar_width\n",
    "\n",
    "    m2_indices=m2_indices[:-1]\n",
    "    indices=indices[:-1]\n",
    "\n",
    "    \n",
    "\n",
    "    print(\"\\nData for Purity\")\n",
    "    plot_on_ax(ax[0], \"blue\", indices, metModel=ModelMets_WP, met_type=\"Purity\")\n",
    "    plot_on_ax(ax[0], \"#EAE7E0\", m2_indices, metModel=ModelMets_NP, met_type=\"Purity\")\n",
    "    print(\"\\nData for Completeness\")\n",
    "    plot_on_ax(ax[1], \"blue\", indices, metModel=ModelMets_WP, met_type=\"Completeness\")\n",
    "    plot_on_ax(ax[1], \"#EAE7E0\", m2_indices, metModel=ModelMets_NP, met_type=\"Completeness\")\n",
    "\n",
    "    ax[0].set_yticks(indices-(bar_width/2))\n",
    "    ax[0].set_yticklabels(clean_class_names(ORDERED_LSST_CLASSES),  fontsize=16, horizontalalignment='right')\n",
    "    ax[1].legend(fontsize=11, loc=\"best\", labelspacing=.2, handlelength=1)\n",
    "\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.subplots_adjust(wspace=0, hspace=0)\n",
    "    plt.savefig(\"../output/custom_figures/prior_comp_combined_AGG.pdf\", bbox_inches='tight')\n",
    "    plt.show() \n",
    "\n",
    "def plot_WP_NP_EPs(ModelMets_NP, ModelMets_WP):\n",
    "    \"\"\"\n",
    "    Plot Empirical Probability Plots (EPs) for with priors vs no-priors, using ModelMets objets\n",
    "    \"\"\"\n",
    "    rc('text', usetex=True) \n",
    "    mpl.rcParams['font.serif'] = ['times', 'times new roman']\n",
    "    mpl.rcParams['font.family'] = 'serif'\n",
    "\n",
    "    class_labels = ORDERED_LSST_CLASSES  \n",
    "    f, ax = plt.subplots(nrows=len(class_labels),\n",
    "                         ncols=2,\n",
    "                         sharex=True, sharey=True,\n",
    "                         figsize=(FIG_WIDTH, 9),\n",
    "                         dpi=DPI)\n",
    "    plot_index = 0\n",
    "    for class_index in range(len(class_labels)):\n",
    "        if plot_index == 0:\n",
    "            # Add titles to top of plots\n",
    "            ax[plot_index][0].set_title(\"Uniform Priors\", fontsize=14)\n",
    "            ax[plot_index][1].set_title(\"Frequency-based Priors\", fontsize=14)\n",
    "\n",
    "    \n",
    "    \n",
    "        ModelMets_NP_preds = np.concatenate(ModelMets_NP.all_results)\n",
    "        ModelMets_NP.class_prob_rates = get_multi_emp_prob_rates(ModelMets_NP_preds,\n",
    "                                                              ModelMets_NP.model.class_labels,\n",
    "                                                              0.2,\n",
    "                                                              ModelMets_NP.model.class_counts)\n",
    "        \n",
    "        ModelMets_WP_preds = np.concatenate(ModelMets_NP.all_results)\n",
    "        ModelMets_WP.class_prob_rates = get_multi_emp_prob_rates(ModelMets_WP_preds,\n",
    "                                                              ModelMets_WP.model.class_labels,\n",
    "                                                              0.2,\n",
    "                                                              ModelMets_WP.model.class_counts)\n",
    "        \n",
    "        class_name = class_labels[class_index]  \n",
    "        plot_model_rates(class_name, ModelMets_NP, ax[plot_index][0])\n",
    "        plot_model_rates(class_name, ModelMets_WP, ax[plot_index][1])\n",
    "\n",
    "        pretty_class_name = clean_class_name(class_name)\n",
    "        ax[plot_index][0].text(-0.2, 0.8, pretty_class_name, fontsize=16) \n",
    "        plot_index += 1\n",
    "\n",
    "    y_indices = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "    y_ticks = [\"10\", \"30\", \"50\", \"70\", \"90\"]\n",
    "    # x and y indices/ticks are the same\n",
    "    plt.xticks(np.arange(5), y_ticks)\n",
    "    plt.yticks(y_indices, y_ticks)\n",
    "    plt.rc('xtick', labelsize=14)\n",
    "    plt.rc('ytick', labelsize=14)\n",
    "\n",
    "    f.text(0.5, 0.06, 'Assigned Probability' + r' $\\pm$10\\%', fontsize=14, ha='center')\n",
    "    f.text(0.02, .5, r'Empirical Probability $\\equiv$ P/Total ($\\%$)',\n",
    "               fontsize=14, va='center', rotation='vertical')\n",
    "\n",
    "    plt.subplots_adjust(wspace=0, hspace=0)\n",
    "\n",
    "    f.savefig(ROOT_DIR + \"/output/custom_figures/merged_metrics_priors_comp_AGG.pdf\", bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelMets_WP = ModelsMets(True)\n",
    "ModelMets_NP = ModelsMets(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine Empirical Probability Plots (averaged over trials)\n",
    "Avg empirical probability plots over trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelMets_NP.range_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelMets_WP.class_prob_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelMets_WP.range_metrics, ModelMets_WP.class_prob_rates  = ModelMets_WP.get_avg_range_metrics()\n",
    "ModelMets_NP.range_metrics, ModelMets_NP.class_prob_rates = ModelMets_NP.get_avg_range_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ModelMets_WP.range_metrics   Map from class name to true_positives, totals\n",
    "# ModelMets_WP.class_prob_rates   Map from class name to pos_class_per_range / total_range_counts (TP/T)\n",
    "\n",
    "\n",
    "\n",
    "plot_WP_NP_EPs(ModelMets_NP, ModelMets_WP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelMets_WP.class_prob_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelMets_NP.class_prob_rates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining single run (of cross fold validation) priors and non priors\n",
    "If only combining a single run of k-fold cross validation (with and without priors) use the following code instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPS_DIR = ROOT_DIR + \"/../../exps/v8_db_runs/new_lsst_tests/\"\n",
    "\n",
    "EXPS_DIR = ROOT_DIR + \"/../../experiments/v8_db/w_wout_priors_4/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_FOLDS = 6\n",
    "MIN_CLASS_SIZE = 3\n",
    "MAX_CLASS_SIZE = 4220\n",
    "\n",
    "modelwithout = MultiModel(cols = mags,\n",
    "                       folds = NUM_FOLDS,\n",
    "                       min_class_size = MIN_CLASS_SIZE,\n",
    "                       max_class_size = MAX_CLASS_SIZE,\n",
    "                       transform_features = True,\n",
    "                       case_code = codes,\n",
    "                       lsst_test= True)\n",
    " \n",
    "multi_wo = load_prev_exp(EXPS_DIR, \n",
    "                         \"Multiclass_Classifier1/\", \n",
    "                         model=modelwithout)\n",
    "\n",
    "multiwith = MultiModel(cols = mags,\n",
    "                       folds = NUM_FOLDS,\n",
    "                       min_class_size = MIN_CLASS_SIZE,\n",
    "                       max_class_size = MAX_CLASS_SIZE,\n",
    "                       transform_features = True,\n",
    "                       case_code = codes,\n",
    "                       priors=True, \n",
    "                       lsst_test= True)\n",
    "multi_w = load_prev_exp(EXPS_DIR,  \n",
    "                        \"Multiclass_Classifier2/\", \n",
    "                        model=multiwith)\n",
    "\n",
    "from mainmodel.helper_compute import *\n",
    "from thex_data.data_consts import * \n",
    "\n",
    "def get_model_stats(model): \n",
    "    N = model.num_runs if model.num_runs is not None else model.num_folds\n",
    "    pc_per_trial = model.get_pc_per_trial(model.results)\n",
    "    ps, cs = model.get_pc_performance(pc_per_trial)\n",
    "    c_baselines, p_baselines = compute_baselines(\n",
    "        model.class_counts, \n",
    "        model.class_labels,\n",
    "        model.get_num_classes(), \n",
    "        model.balanced_purity,  \n",
    "        model.class_priors)\n",
    "    p_intvls, c_intvls = compute_confintvls(pc_per_trial, model.class_labels, model.balanced_purity)\n",
    "    \n",
    "    p_class_names, p_metrics, p_b, p_intvls = get_ordered_metrics(\n",
    "            ps,\n",
    "            p_baselines,\n",
    "            p_intvls)\n",
    "    \n",
    "    c_class_names, c_metrics, c_b, c_intvls = get_ordered_metrics(\n",
    "            cs,\n",
    "            c_baselines,\n",
    "            c_intvls)\n",
    "    \n",
    "    m_stats = {\"Purity\":[p_class_names,p_metrics,p_b,p_intvls],\n",
    "              \"Completeness\":[c_class_names, c_metrics, c_b, c_intvls]}\n",
    "    return m_stats\n",
    "         \n",
    "def plot_m(ax, indices, errs, baselines, metrics, name, color):\n",
    "\n",
    "#     bar_width=0.1\n",
    "    bar_width= 1 / (len(indices)  *2)\n",
    "    if len(indices)>6:\n",
    "        capsize=4\n",
    "    else:\n",
    "        capsize=7\n",
    "    ax.barh(y=indices, \n",
    "            width=metrics, \n",
    "            height=bar_width, \n",
    "            xerr=errs,\n",
    "            capsize=capsize, \n",
    "            linewidth=0.5,\n",
    "            edgecolor=BAR_EDGE_COLOR, \n",
    "            ecolor=INTVL_COLOR, \n",
    "            color=color,\n",
    "            label=name)\n",
    "    for index, baseline in enumerate(baselines):\n",
    "        y_val = indices[index]\n",
    "        ax.vlines(x=baseline,\n",
    "                   ymin=y_val - (bar_width / 2),\n",
    "                   ymax=y_val + (bar_width / 2),\n",
    "                   linewidth=2,\n",
    "               linestyles=(0, (1, 1)), colors=BSLN_COLOR)\n",
    "            \n",
    "def plot_priors_together(axis, WP_model, WP_model_stats, NP_model, NP_model_stats, plot_type):\n",
    "    m1_class_names, m1_metrics, m1_b, m1_intvls  = WP_model_stats[plot_type]\n",
    "    m2_class_names, m2_metrics, m2_b, m2_intvls  = NP_model_stats[plot_type]\n",
    "    WITHOUT_COLOR = \"#808080\" # without \n",
    "    WITH_COLOR = \"#80b3ff\" # with\n",
    "        \n",
    "    m1_errs = prep_err_bars(m1_intvls, m1_metrics) \n",
    "    m1_indices = np.linspace(0,1,len(multi_w.class_labels))\n",
    "    bar_width= 1 / (len(m1_indices) *2)\n",
    "    m2_indices = m1_indices - bar_width\n",
    "     \n",
    "    L = len(multi_w.class_labels)\n",
    "    m2_indices = np.linspace(0,1,L+1)\n",
    "    m1_indices = m2_indices + bar_width\n",
    "    \n",
    "    m2_indices=m2_indices[:-1]\n",
    "    m1_indices=m1_indices[:-1]\n",
    "    \n",
    "    # Print performance per class Plus/minus 95% confidence\n",
    "    print(\"\\n\"+plot_type+\" above-random stats:\")\n",
    "    for index, cn in enumerate(m1_class_names):\n",
    "        print(\"Class: \"+ cn)\n",
    "        baseline = m1_b[index]\n",
    "        cur_met_W= m1_metrics[index]\n",
    "        d_W = (m1_intvls[index][1]-m1_intvls[index][0])/2\n",
    "        RT = 3\n",
    "        if cur_met_W-d_W>baseline:\n",
    "            print(\" WP : \" + str(round(cur_met_W,RT)*100) + \"\\% \\pm \" + str(round(d_W,RT)*100) +\"\\%\")\n",
    "        \n",
    "        cur_met_WO = m2_metrics[index]\n",
    "        d_WO = (m2_intvls[index][1]-m2_intvls[index][0])/2\n",
    "        if cur_met_WO-d_WO>baseline:\n",
    "            print(\" NP : \" + str(round(cur_met_WO,RT)*100) + \"\\% \\pm \" + str(round(d_WO,RT)*100) +\"\\%\")\n",
    "    \n",
    "    m2_errs = prep_err_bars(m2_intvls, m2_metrics)\n",
    "    \n",
    "    plot_m(ax=axis, indices=m1_indices, errs=m2_errs, baselines=m2_b, \n",
    "           metrics=m2_metrics, color=WITHOUT_COLOR, name=NP_model.name)\n",
    "    plot_m(ax=axis, indices=m2_indices, errs=m1_errs, baselines=m1_b, \n",
    "           metrics=m1_metrics, color=WITH_COLOR, name=WP_model.name)\n",
    "    # Figure formatting\n",
    "    axis.set_xlim(0, 1)\n",
    "    axis.set_xticks([0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9, 1])\n",
    "    axis.set_xticklabels([\"0\", \"\", \"20\", \"\", \"40\", \"\", \"60\", \"\", \"80\", \"\", \"\"], fontsize=14)\n",
    "    axis.set_xlabel(plot_type+\" (\\%)\", fontsize=16)  \n",
    "    if plot_type == \"Purity\": \n",
    "#         ylabel_points = [0.15, 0.35, 0.55, 0.75, 0.95]\n",
    "        \n",
    "        ylabel_points = m1_indices-(bar_width/2)\n",
    "        axis.set_yticks(ylabel_points)\n",
    "        axis.set_yticklabels(clean_class_names(m1_class_names),  fontsize=16,\n",
    "                       horizontalalignment='right')\n",
    "    else:\n",
    "        axis.legend(fontsize=11, loc=\"best\", labelspacing=.2, handlelength=1)\n",
    "\n",
    "\n",
    "f, ax = plt.subplots(nrows=1, ncols=2,\n",
    "                     sharex=True, sharey=True,\n",
    "                     figsize=(6,4),  dpi=500)\n",
    "\n",
    "rc('text', usetex=True)\n",
    "mpl.rcParams['font.serif'] = ['times', 'times new roman']\n",
    "mpl.rcParams['font.family'] = 'serif'\n",
    "\n",
    "print(\"\\n\\n --------------- WITH Priors -------------------- \\n\")\n",
    "WP_stats =  get_model_stats(multi_w) # WP = with priors\n",
    "\n",
    "print(\"\\n\\n --------------- WITHOUT Priors -------------------- \\n\")\n",
    "NP_stats = get_model_stats(multi_wo) # NP = no priors\n",
    "multi_w.name = \"Freq-based priors\"\n",
    "multi_wo.name = \"Uniform priors\"\n",
    "\n",
    "\n",
    "plot_priors_together(axis=ax[0],\n",
    "                     WP_model=multi_w, \n",
    "                     WP_model_stats=WP_stats, \n",
    "                     NP_model=multi_wo, \n",
    "                     NP_model_stats=NP_stats, \n",
    "                     plot_type=\"Purity\")\n",
    "plot_priors_together(axis=ax[1],\n",
    "                     WP_model=multi_w, \n",
    "                     WP_model_stats=WP_stats, \n",
    "                     NP_model=multi_wo, \n",
    "                     NP_model_stats=NP_stats, \n",
    "                     plot_type=\"Completeness\")\n",
    "\n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "plt.savefig(\"../output/custom_figures/prior_comp_combined.pdf\", bbox_inches='tight')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine Empirical Probability Plots \n",
    "Just get empirical probability plots for K-F CV and plot together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from thex_data.data_consts import ROOT_DIR\n",
    "from mainmodel.helper_plotting import *\n",
    "\n",
    "multi_w.range_metrics = multi_w.compute_probability_range_metrics(\n",
    "        multi_w.results, bin_size=0.2)\n",
    "multi_wo.range_metrics = multi_wo.compute_probability_range_metrics(\n",
    "        multi_wo.results, bin_size=0.2)\n",
    "\n",
    "\n",
    "rc('text', usetex=True) \n",
    "mpl.rcParams['font.serif'] = ['times', 'times new roman']\n",
    "mpl.rcParams['font.family'] = 'serif'\n",
    "\n",
    "# class_labels = multi_w.class_labels\n",
    "# multi_w.class_labels = [\"Unspecified Ia\", \"Ia-91bg\", \"Ibc\", \"Unspecified II\",  \"TDE\" ]\n",
    "class_labels =ORDERED_LSST_CLASSES # ['Unspecified Ia', 'Ia-91bg', 'Ibc', 'II (cust.)',  'SLSN-I', 'TDE']\n",
    "num_classes = len(multi_w.class_labels) \n",
    "f, ax = plt.subplots(nrows=num_classes,\n",
    "                     ncols=2,\n",
    "                     sharex=True, sharey=True,\n",
    "                     figsize=(FIG_WIDTH, 9),\n",
    "                     dpi=DPI)\n",
    "plot_index = 0\n",
    "for class_index in range(len(class_labels)):\n",
    "    if plot_index == 0:\n",
    "        # Add titles to top of plots\n",
    "        ax[plot_index][0].set_title(\"Uniform Priors\", fontsize=14)\n",
    "        ax[plot_index][1].set_title(\"Frequency-based Priors\", fontsize=14)\n",
    "        \n",
    "\n",
    "    class_name = class_labels[class_index]  \n",
    "    plot_model_rates(class_name, multi_wo, ax[plot_index][0])\n",
    "    plot_model_rates(class_name, multi_w, ax[plot_index][1])\n",
    "    \n",
    "    pretty_class_name = clean_class_name(class_name)\n",
    "    ax[plot_index][0].text(-0.2, 0.8, pretty_class_name, fontsize=16) \n",
    "    plot_index += 1\n",
    "\n",
    "y_indices = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "y_ticks = [\"10\", \"30\", \"50\", \"70\", \"90\"]\n",
    "# x and y indices/ticks are the same\n",
    "plt.xticks(np.arange(5), y_ticks)\n",
    "plt.yticks(y_indices, y_ticks)\n",
    "plt.rc('xtick', labelsize=14)\n",
    "plt.rc('ytick', labelsize=14)\n",
    "\n",
    "f.text(0.5, 0.06, 'Assigned Probability' + r' $\\pm$10\\%', fontsize=14, ha='center')\n",
    "f.text(0.02, .5, r'Empirical Probability $\\equiv$ TP/Total ($\\%$)',\n",
    "           fontsize=14, va='center', rotation='vertical')\n",
    "\n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "\n",
    "f.savefig(ROOT_DIR + \"/output/custom_figures/merged_metrics_priors_comp.pdf\", bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f, ax = plt.figure(figsize=(4, 4), dpi=200)\n",
    "\n",
    "f, ax = plt.subplots(nrows=1,\n",
    "                     ncols=1,\n",
    "#                      sharex=True, sharey=True,\n",
    "                     figsize=(4,4),\n",
    "                     dpi=280)\n",
    "\n",
    "multi_range_metrics = multi_w.compute_probability_range_metrics(\n",
    "        multi_w.results)\n",
    "mirror_ax = plot_model_curves(\n",
    "            \"TDE\", multi_w, multi_range_metrics, ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for class_index, class_name in enumerate(multi_w.class_labels): \n",
    "class_name = \"TDE\"\n",
    "results = np.concatenate(multi_w.results)\n",
    "label_index = len(multi_w.class_labels)\n",
    "TP_counts = [0,0,0,0,0,0,0,0,0,0]\n",
    "total_counts = [0,0,0,0,0,0,0,0,0,0]\n",
    "for row in results:\n",
    "    labels = row[label_index]\n",
    "    is_class = multi_w.is_class(class_name, labels)\n",
    "\n",
    "    # Get class index of max prob; exclude last column since it is label\n",
    "    max_class_prob = np.max(row[: len(row) - 1])\n",
    "    max_class_index = np.argmax(row[: len(row) - 1])\n",
    "    max_class_name = multi_w.class_labels[max_class_index]\n",
    "    \n",
    "    bins = np.arange(0, 1.01, 0.1)\n",
    "    counts, ranges = np.histogram([max_class_prob], bins=bins) \n",
    "\n",
    "    if max_class_name == \"TDE\":\n",
    "        total_counts=np.add(counts, total_counts)\n",
    "        if is_class:\n",
    "            TP_counts=np.add(counts, TP_counts) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example output plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mainmodel.helper_compute import *\n",
    "from thex_data.data_consts import *\n",
    "import utilities.utilities as thex_utils\n",
    "\n",
    "model = MultiModel(folds=3,\n",
    "                   min_class_size = 40,   \n",
    "                   priors = True,\n",
    "                   transform_features = True,\n",
    "                   cols = mags, \n",
    "                   lsst_test=True)  \n",
    "model.run_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Want to find 16 quality examples\n",
    "\n",
    "8: good & correct examples with wide dist\n",
    "    correct examples for II P, IIn, TDE, GRB, Ib (unspec.)\n",
    "\n",
    "4: not 'good' but correct examples (high prob on Ia or something)\n",
    "\n",
    "4: incorrect, but benefit from empirical prob plots (so look for maybe Ia with probs 20-49%, where Ia IS the correct answer but something else won; or could show 4 GRB plots with probs >80%, and half are correct but half are not- this exemplifies its 50% empriical prob at the range. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultiModel(folds=3,\n",
    "                   min_class_size = 40,   \n",
    "                   priors = True,\n",
    "                   transform_features = True,\n",
    "                   cols = mags, \n",
    "                   lsst_test=True)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.run_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_on_ax(axis, model, do_index, preds):\n",
    "    \"\"\"\n",
    "    Plot example on given ax\n",
    "    \"\"\"\n",
    "#     if do_index == -1:\n",
    "        # Custom sample ; put Ia-91bg prob at range \n",
    "    row = preds[do_index] \n",
    "    labels = row[len(row) - 1]\n",
    "    true_class_index = None\n",
    "\n",
    "    for class_index, class_name in enumerate(model.class_labels):\n",
    "        if class_name in thex_utils.convert_str_to_list(labels):\n",
    "            true_class_index = class_index\n",
    "\n",
    "\n",
    "    ACC = \"#ccccff\"# actual class color, light blue\n",
    "    DCC = \"#0000cc\" # default class color, dark blue\n",
    "\n",
    "    colors = [DCC] * len(model.class_labels)\n",
    "    colors[true_class_index] = ACC\n",
    "    probabilities = row[0:len(row) - 1]  \n",
    "    bar_size = 0.05\n",
    "    x_indices = np.linspace(0,\n",
    "                            len(model.class_labels) * bar_size,\n",
    "                            len(model.class_labels))\n",
    "    axis.bar(x=x_indices, height=probabilities,  \n",
    "             width=bar_size, \n",
    "             color=colors, edgecolor='black')\n",
    "    axis.tick_params(axis=\"y\",direction=\"in\")\n",
    "    axis.set_ylim([0, 1]) \n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.concatenate(model.results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  use following to find good samples.\n",
    "use_indices = []\n",
    "USE_CLASS = \"Unspecified Ia\"\n",
    "for index, row in enumerate(preds):\n",
    "    labels = row[len(row) - 1]\n",
    "    if USE_CLASS in labels:\n",
    "        if row[0] > 0.5 and row[0] <0.65:\n",
    "            use_indices.append(index)\n",
    "\n",
    "for do_index in use_indices:\n",
    "    f, ax = plt.subplots(nrows=1, ncols=1,  figsize=(4,2), dpi=DPI)\n",
    "    plot_on_ax(ax, model, do_index, do_index, preds) \n",
    "    bar_size = 0.05\n",
    "    x_indices = np.linspace(0, len(model.class_labels) * bar_size, len(model.class_labels))\n",
    "    ax.set_xticks(ticks=x_indices)\n",
    "    xticksize = 11\n",
    "    pretty_class_names = clean_class_names(model.class_labels) \n",
    "    ax.set_xticklabels(labels=pretty_class_names, fontsize=xticksize, rotation = -90)\n",
    "\n",
    "\n",
    "    plt.savefig(model.dir + \"/examples/\"+USE_CLASS+\"/sample_\" + str(do_index) + \".pdf\", \n",
    "                bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "indices = [[2964, 2965, 6032, 9136],  # Good , 9140\n",
    "          [2400, 2394, 9103, 9140], # good too \n",
    "          [2974, 3031, 2378, 2461]]  # wrong, low probs\n",
    "#           [xx, xx, xx, xx]] #wrong but empirical probs would help a lot.\n",
    "rows, cols = np.array(indices).shape\n",
    "\n",
    "f, ax = plt.subplots(nrows=rows,\n",
    "                     ncols=cols,\n",
    "                     sharex=True, sharey=True,\n",
    "                     figsize=(6,5),\n",
    "                     dpi=DPI)\n",
    "do_index = 0\n",
    "example_num = 1\n",
    "for row_index, index_set in enumerate(indices): \n",
    "    cur_ax = ax[row_index]\n",
    "    for plot_index, do_index in enumerate(index_set): \n",
    "        plot_on_ax(cur_ax[plot_index], model, do_index, preds)\n",
    "        if str(example_num) == \"1\":\n",
    "            cur_ax[plot_index].text(.17,.8,  str(example_num), style='italic',fontsize=15)\n",
    "        else:\n",
    "            cur_ax[plot_index].text(.23,.8,  str(example_num), style='italic',fontsize=15)\n",
    "        example_num+=1\n",
    "        \n",
    "        if plot_index==0:\n",
    "            yticks = np.arange(0,1.2,.2)\n",
    "            cur_ax[plot_index].set_yticks(ticks=yticks)\n",
    "            cur_ax[plot_index].set_yticklabels(labels=[str(int(i*100)) for i in yticks],\n",
    "                                               fontsize=12)\n",
    "\n",
    "\n",
    "f.text(.04, 0.4, 'Probability (\\%)', fontsize=14, ha='center', rotation =90)\n",
    "bar_size = 0.05\n",
    "x_indices = np.linspace(0, len(model.class_labels) * bar_size,\n",
    "                            len(model.class_labels))\n",
    "\n",
    "\n",
    "pretty_class_names = clean_class_names(model.class_labels) \n",
    "xticksize = 13\n",
    "for i in range(cols):\n",
    "    ax[rows-1][i].set_xticks(ticks=x_indices)\n",
    "    ax[rows-1][i].set_xticklabels(labels=pretty_class_names, fontsize=xticksize, rotation = -90)\n",
    "plt.subplots_adjust(wspace=0, hspace=0, left=0.1)\n",
    "\n",
    "plt.savefig(\"../output/custom_figures/lsst_examples.pdf\", bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('../output/custom_figures/full_model_data.pickle', 'wb') as handle:\n",
    "    pickle.dump(model, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Archive\n",
    "### Probability purity/completeness plots \n",
    "Old way of using normal purity and completeness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def plot_model_curves(class_name, model, ax):\n",
    "    \"\"\"\n",
    "    Plots rates for this model/class on axis, with annotations\n",
    "    \"\"\"\n",
    "    purities, comps = get_pc_per_range(model, class_name)\n",
    "\n",
    "    def plot_axis(ax, data, color):\n",
    "        \"\"\"\n",
    "        Plot data on axis in certain color\n",
    "        \"\"\"\n",
    "        x_indices = np.linspace(0, 1, 11)[:-1]\n",
    "\n",
    "        \n",
    "        print(\"Data: \" + str(data))\n",
    "        # Do not plot points whose data is 0; so that we may distinguish between true 0 purity and having no data. \n",
    "        # If both purity and completeness are 0 we do not plot.\n",
    "        total_range_counts=model.range_metrics[class_name][1]\n",
    "        keep_indices = []\n",
    "        keep_data = []\n",
    "        for i, t in enumerate(total_range_counts):\n",
    "            if t != 0:\n",
    "                keep_indices.append(x_indices[i])\n",
    "                keep_data.append(data[i])\n",
    "                \n",
    "        ax.scatter(keep_indices, keep_data, color=color, s=4)\n",
    "        ax.plot(keep_indices, keep_data, color=color, linewidth=2)\n",
    "        ax.set_yticks([])  # same for y ticks\n",
    "        ax.set_ylim([0, 1])\n",
    "\n",
    "    print(\"\\n\\n P-C metrics for : \" + class_name)\n",
    "    plot_axis(ax, comps, C_BAR_COLOR)\n",
    "    ax2 = ax.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "    ax2.set_ylim([0, 1])\n",
    "    plot_axis(ax2, purities, P_BAR_COLOR)\n",
    "    for axis in ['top', 'bottom', 'left', 'right']:\n",
    "        ax.spines[axis].set_linewidth(1.5)\n",
    "    return ax2\n",
    "\n",
    "\n",
    "def plot_pc_curves_together(binary_model, ova_model, multi_model, indices=None):\n",
    "    \"\"\"\n",
    "    Plot class versus probability rates of all three classifiers together\n",
    "    :param indices: class indices to plot\n",
    "    \"\"\"\n",
    "    class_labels = ova_model.class_labels\n",
    "    num_classes = len(ova_model.class_labels)\n",
    "    if indices is not None:\n",
    "        num_classes = len(indices)\n",
    "    f, ax = plt.subplots(nrows=num_classes,\n",
    "                         ncols=3,\n",
    "                         sharex=True, sharey=True,\n",
    "                         figsize=(FIG_WIDTH, 10),\n",
    "                         dpi=DPI)\n",
    "\n",
    "    y_indices = [0, 0.2, 0.4, 0.6, 0.8, 1]\n",
    "    y_ticks = [\"0\", \"20\", \"40\", \"60\", \"80\", \"\"]\n",
    "\n",
    "    plot_index = 0\n",
    "    for class_index in range(len(class_labels)):\n",
    "        if indices is not None and class_index not in indices:\n",
    "            continue\n",
    "\n",
    "        if plot_index == 0:\n",
    "            # Add titles to top of plots\n",
    "            ax[plot_index][0].set_title(\"Binary\", fontsize=TICK_S)\n",
    "            ax[plot_index][1].set_title(\"OVA\", fontsize=TICK_S)\n",
    "            ax[plot_index][2].set_title(\"Multi\", fontsize=TICK_S)\n",
    "\n",
    "        class_name = class_labels[class_index]\n",
    "        print(\"Binary model\")\n",
    "        plot_model_curves(class_name, binary_model, ax[plot_index][0])\n",
    "        print(\"OVA model\")\n",
    "        plot_model_curves(class_name, ova_model, ax[plot_index][1])\n",
    "        print(\"KDE Multi model\")\n",
    "        mirror_ax = plot_model_curves(class_name, multi_model, ax[plot_index][2])\n",
    "\n",
    "        ax[plot_index][0].set_yticks(ticks=y_indices)\n",
    "        ax[plot_index][0].set_yticklabels(labels=y_ticks, color=P_BAR_COLOR)\n",
    "        mirror_ax.set_yticks(ticks=y_indices)\n",
    "        mirror_ax.set_yticklabels(labels=y_ticks, color=C_BAR_COLOR)\n",
    "        ax[plot_index][0].tick_params(axis='both', direction='in', labelsize=10)\n",
    "        ax[plot_index][1].tick_params(axis='both', direction='in')\n",
    "        ax[plot_index][2].tick_params(axis='both', direction='in', labelsize=10)\n",
    "\n",
    "        mpl.rcParams['font.serif'] = ['times', 'times new roman']\n",
    "        mpl.rcParams['font.family'] = 'serif'\n",
    "        pretty_class_name = clean_class_name(class_name)\n",
    "        ax[plot_index][0].text(0, 0.85, pretty_class_name, fontsize=14)\n",
    "        plot_index += 1\n",
    "\n",
    "    x_indices = np.linspace(0, 1, 11)[:-1]\n",
    "\n",
    "    plt.xticks(x_indices, [\"\", \"10\", \"\", \"30\", \"\", \"50\", \"\", \"70\", \"\", \"90\"])\n",
    "    rc('text', usetex=True)\n",
    "    f.text(0.5, 0.08, r'Probability $\\geq$X\\%', fontsize=TICK_S, ha='center')\n",
    "    f.text(0.03, .5, 'Purity (\\%)',\n",
    "           fontsize=TICK_S, va='center', rotation='vertical', color=P_BAR_COLOR)\n",
    "    f.text(0.98, .5, 'Completeness (\\%)',\n",
    "           fontsize=TICK_S, va='center', rotation='vertical', color=C_BAR_COLOR)\n",
    "\n",
    "    plt.subplots_adjust(wspace=0, hspace=0)\n",
    "\n",
    "    f.savefig(\"../output/custom_figures/merged_pc_curves_\" +\n",
    "              str(indices) + \".pdf\", bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "ova_model.range_metrics = ova_model.compute_probability_range_metrics(\n",
    "        ova_model.results, bin_size=0.1)\n",
    "binary_model.range_metrics = binary_model.compute_probability_range_metrics(\n",
    "        binary_model.results, bin_size=0.1)\n",
    "multi_model.range_metrics = multi_model.compute_probability_range_metrics(\n",
    "        multi_model.results, bin_size=0.1)\n",
    "\n",
    "plot_pc_curves_together(binary_model, ova_model, multi_model, indices=[0,1,2,3,4,5])\n",
    "plot_pc_curves_together(binary_model, ova_model, multi_model, indices=[6,7,8,9,10,11])\n",
    "plot_pc_curves_together(binary_model, ova_model, multi_model, indices=[0,7,8,9,10,11])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "THEx env",
   "language": "python",
   "name": "thex-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
