{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Running KDE vs. NADE\n",
    "\n",
    "Use given training/testing data that has already been processed to match that used by NADE\n",
    "\n",
    "Have 1 set for training and 1 for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline   \n",
    "from models.multi_model.multi_model import MultiModel\n",
    "\n",
    "\n",
    "mags = [\"g_mag\",  \"r_mag\", \"i_mag\", \"z_mag\", \"y_mag\",\n",
    "        \"W1_mag\", \"W2_mag\",\n",
    "        \"J_mag\", \"K_mag\", \"H_mag\"]\n",
    " \n",
    "# initialize arbitrarily because we will replace data\n",
    "# mags = [\"g_mag\"]\n",
    "model = MultiModel(\n",
    "       cols = mags,\n",
    "       folds = 3, \n",
    "       min_class_size = 40, \n",
    "       transform_features = True\n",
    "       )\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run on NADE-used data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from thex_data.data_transform import scale_data\n",
    "# X_train, X_test = scale_data(X_train, X_test)\n",
    "\n",
    "# model.train_model(X_train, y_train)\n",
    "\n",
    "# probabilities = model.get_all_class_probabilities(X_test)\n",
    "# probs = pd.DataFrame(probabilities, columns = model.class_labels)\n",
    "# probs.to_csv(test_path + \"OUTPUT/\" + \"kde_probabilities.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from thex_data.data_transform import scale_data\n",
    "\n",
    "def get_performance(lls, y, classes):\n",
    "    class_mets = get_metrics(lls, y)\n",
    "\n",
    "    # Purity TP / (TP+FP)\n",
    "    purities = OrderedDict()\n",
    "    for cname in classes:\n",
    "        m = class_mets[cname]\n",
    "        p = m[\"TP\"] / (m[\"TP\"] + m[\"FP\"] + 0.0)\n",
    "        purities[cname] = p\n",
    "        print(cname + ' purity : ' + str(round(p * 100, 1)) + \"%\")\n",
    "\n",
    "    total_TPs = 0\n",
    "    for cname in classes:\n",
    "        total_TPs += class_mets[cname][\"TP\"]\n",
    "    acc = total_TPs / (y.shape[0] + 0.0)\n",
    "    print('Accuracy : ' + str(round(acc * 100, 1)) + \"%\")\n",
    "\n",
    "    return purities, acc\n",
    "\n",
    "def get_metrics(lls, y):\n",
    "\n",
    "    class_names = list(lls)\n",
    "\n",
    "    label_col = list(y)[0] \n",
    "\n",
    "    # Map from class name to TP, FP, FN, and TN rates in dict.\n",
    "    class_mets = {cn: {\"TP\": 0, \"TN\": 0, \"FP\": 0, \"FN\": 0} for cn in class_names}\n",
    "\n",
    "    for class_name in class_names:\n",
    "        for index, ll in lls.iterrows():\n",
    "            label = str(y.iloc[index][label_col])\n",
    "            values = ll.tolist()\n",
    "            max_class_index = values.index(max(values))\n",
    "            pred_class = class_names[max_class_index]\n",
    "\n",
    "            # this class is the label, and we predicted it as max\n",
    "            if label == class_name and pred_class == label:\n",
    "                class_mets[class_name][\"TP\"] += 1\n",
    "\n",
    "            # this class is not the label, but it was the max likelihood\n",
    "            elif label != class_name and pred_class == class_name:\n",
    "                class_mets[class_name][\"FP\"] += 1\n",
    "\n",
    "            # this class is not the label, and its not the max (TN)\n",
    "            elif label != class_name and pred_class != class_name:\n",
    "                class_mets[class_name][\"TN\"] += 1\n",
    "\n",
    "            # this class is label, but its not the max (FN)\n",
    "            elif label == class_name and pred_class != class_name:\n",
    "                class_mets[class_name][\"FN\"] += 1\n",
    "\n",
    "    return class_mets\n",
    "\n",
    "\n",
    "def run_for_dataset(X_train, y_train, X_test, y_test):\n",
    "    \n",
    "#     X_train, X_test = scale_data(X_train, X_test)\n",
    "    \n",
    "    model.train_model(X_train, y_train)\n",
    "\n",
    "    probabilities = model.get_all_class_probabilities(X_test)\n",
    "    probs = pd.DataFrame(probabilities, columns = model.class_labels)\n",
    "#     probs.to_csv(test_path + \"OUTPUT/\" + \"kde_probabilities.csv\", index=False)\n",
    "    \n",
    "    purities, acc=get_performance(probs, y_test, model.class_labels)\n",
    "    return purities, acc\n",
    "\n",
    "def get_mean_stdev(l):\n",
    "    \"\"\"\n",
    "    Get mean and standard deviation of list\n",
    "    \"\"\"\n",
    "    arr = np.array(l)\n",
    "    mean = np.average(arr)\n",
    "    mean = str(round(mean * 100, 1)) + \"%\"\n",
    "\n",
    "    stdev = np.std(arr)\n",
    "    stdev = str(round(stdev * 100, 1))\n",
    "    return mean, stdev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimate performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################\n",
    "\n",
    "# model.class_labels = ['dog', 'cat', 'mouse']\n",
    "model.class_labels = ['Unspecified Ia', 'Unspecified II']\n",
    "class_label = \"transient_type\" #\"quality\"\n",
    "# model.class_labels = [\"4\",\"5\",\"6\",\"7\"]\n",
    "runs=10\n",
    "\n",
    "############################################################\n",
    "\n",
    "\n",
    "test_path=\"/Users/marina/Documents/PhD/research/astro_research/data/testing/\"\n",
    "dpath = test_path+\"PROCESSED_DATA/\"\n",
    "\n",
    "# Set X and y to training data given\n",
    "X_train = pd.read_csv(dpath + \"train_X.csv\")\n",
    "y_train = pd.read_csv(dpath + \"train_y.csv\")\n",
    "\n",
    "X_test = pd.read_csv(dpath + \"test_X.csv\")\n",
    "y_test = pd.read_csv(dpath + \"test_y.csv\")\n",
    "\n",
    "# Rename column label\n",
    "if class_label != 'transient_type':\n",
    "    y_train['transient_type']= y_train[class_label].map(str)\n",
    "    y_train.drop(columns=class_label,inplace=True)\n",
    "    y_test['transient_type']= y_test[class_label].map(str)\n",
    "    y_test.drop(columns=class_label,inplace=True)\n",
    "\n",
    "model.X = X_train\n",
    "model.y = y_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ps = []\n",
    "all_accs = []\n",
    "for i in range(runs):\n",
    "    purities, acc = run_for_dataset(X_train.copy(deep=True), \n",
    "                                    y_train.copy(deep=True), \n",
    "                                    X_test.copy(deep=True), \n",
    "                                    y_test.copy(deep=True))\n",
    "    all_ps.append(purities)\n",
    "    all_accs.append(acc)\n",
    "\n",
    "avg, stdev = get_mean_stdev(all_accs)\n",
    "print(\"Average accuracy \" + avg + u\"\\u00B1\" + stdev)\n",
    "\n",
    "for class_name in model.class_labels:\n",
    "    class_p = []\n",
    "    for p_map in all_ps:\n",
    "        class_p.append(p_map[class_name])\n",
    "    avg, stdev = get_mean_stdev(class_p)\n",
    "    print(\"Purity \" + class_name + \" :\" + avg + u\"\\u00B1\" + stdev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "THEx env (py3env)",
   "language": "python",
   "name": "thexkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
